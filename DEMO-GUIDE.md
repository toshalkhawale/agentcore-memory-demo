# Agentcore Memory Demo - Presentation Guide
## ACD Ahmedabad 2026

## Overview
This demo showcases a dual-memory architecture for AI agents using the "Knight of the Seven Kingdoms" persona.

## Visual Memory Indicators üé®

The web interface features **color-coded text** to visually distinguish between memory sources:

- **üîµ Blue Text** = Short-Term Memory (STM)
  - Recent conversation context
  - Immediate responses based on current dialogue
  - Disappears when STM overflows (after ~3-4 exchanges with 200 token limit)

- **üü¢ Green Text** = Long-Term Memory (LTM)
  - Core memories retrieved via semantic search
  - Persistent knowledge about Ser Duncan's past
  - Always available regardless of conversation length

**Why This Matters:**
- Makes the dual-memory architecture immediately visible to the audience
- Clearly demonstrates when each memory system is being used
- Shows the complementary nature of STM and LTM in real-time
- Perfect for educational demonstrations and presentations

**Legend Display:**
A color legend appears at the top of the chat interface showing:
- Blue box: "STM (Short-Term Memory - Recent Context)"
- Green box: "LTM (Long-Term Memory - Core Memories)"

## Key Concepts

### 1. Short-Term Memory (STM)
- **Purpose**: Maintains recent conversation context
- **Implementation**: Sliding window buffer with token limits
- **Capacity**: 200 tokens (configurable via STM_MAX_TOKENS)
- **Behavior**: Automatically removes oldest messages when capacity reached
- **Use Case**: Keeps track of the immediate conversation flow, pronouns, and recent topics

**Example Flow:**
```
Message 1: "Who are you?" (15 tokens) ‚Üí STM: 15/200
Message 2: "I am Ser Duncan..." (45 tokens) ‚Üí STM: 60/200
Message 3: "Tell me about Egg" (20 tokens) ‚Üí STM: 80/200
Message 4: "He is my squire..." (35 tokens) ‚Üí STM: 115/200
Message 5: "What about honor?" (18 tokens) ‚Üí STM: 133/200
Message 6: "Honor is everything..." (40 tokens) ‚Üí STM: 173/200
Message 7: "Tell me more" (15 tokens) ‚Üí STM: 188/200
Message 8: "A knight must..." (25 tokens) ‚Üí STM: 213/200
‚Üí Oldest message removed ‚Üí STM: 198/200
```

### 2. Long-Term Memory (LTM)
- **Purpose**: Persistent knowledge storage and semantic retrieval
- **Implementation**: Vector database (FAISS) with semantic search
- **Storage**: Embeddings using Sentence Transformers (all-MiniLM-L6-v2)
- **Retrieval**: Similarity-based search for relevant memories
- **Persistence**: Saved to disk, survives application restarts
- **Initial Load**: 20 core memories about Ser Duncan's life and adventures

**Memory Categories:**
- `origin`: Backstory and how he became a knight
- `defining_moment`: Key events that shaped his character
- `relationships`: Important people in his life
- `values`: His beliefs and principles
- `identity`: Who he is and how he sees himself
- `possessions`: His belongings and their significance
- `personality`: His traits and characteristics

**Example LTM Entries:**
```
Memory 1: "I was knighted by Ser Arlan of Pennytree on his deathbed..."
  Category: origin
  Importance: 10
  Embedding: [0.234, -0.123, 0.456, ...]

Memory 2: "At the Tourney at Ashford Meadow, I defended Tanselle..."
  Category: defining_moment
  Importance: 10
  Embedding: [0.123, 0.234, -0.345, ...]
```

### 3. Memory Retrieval Strategy

#### Recency-Based (STM)
```
Recent messages ‚Üí Immediate context
Advantages: Fast, maintains conversation flow
Limitations: Limited capacity, loses old context
```

#### Relevance-Based (LTM)
```
User query ‚Üí Embedding ‚Üí Similarity search ‚Üí Top-K memories
Advantages: Retrieves semantically relevant information
Limitations: May miss exact matches, requires good embeddings
```

#### Hybrid Approach (Our Implementation)
```
1. User sends message
2. Add to STM
3. Query LTM for relevant memories
4. Combine STM + LTM for context
5. Generate response
6. Store important interactions in LTM
```

## Demo Scenarios

### Scenario 1: STM in Action - Conversation Continuity
**Goal**: Demonstrate how STM maintains recent context and handles overflow

**Visual Cue**: Watch for **blue text** (STM) in responses showing recent context

**Step-by-Step:**
```
1. User: "Who are you?"
   ‚Üí Agent: "I am Ser Duncan the Tall, a hedge knight..."
   ‚Üí Blue text: Immediate response from conversation context
   ‚Üí STM: 1 message (user) + 1 message (agent) ‚âà 60 tokens
   ‚Üí Stats: STM: 60/200 tokens, LTM: 20 memories

2. User: "What did you just tell me?"
   ‚Üí Agent: References the introduction from STM
   ‚Üí Blue text: Shows agent remembers recent conversation
   ‚Üí STM: 4 messages ‚âà 120 tokens

3. User: "Tell me more about being a hedge knight"
   ‚Üí Agent: Responds with details
   ‚Üí Blue text: Conversational response
   ‚Üí STM: 6 messages ‚âà 180 tokens
   ‚Üí Shows: Approaching capacity

4. User: "Where do you travel?"
   ‚Üí Agent: Responds about traveling the Seven Kingdoms
   ‚Üí Blue text: Recent context response
   ‚Üí STM: 8 messages ‚âà 220 tokens
   ‚Üí Oldest messages start being removed
   ‚Üí Shows: Automatic memory management (overflow!)

5. User: "What was the first thing I asked you?"
   ‚Üí Agent: May not remember (removed from STM)
   ‚Üí Shows: STM limitations - only ~3-4 exchanges fit in 200 tokens
   ‚Üí Demonstrates why LTM is essential
```

**Key Observation**: With only 200 tokens, overflow happens quickly (after 3-4 exchanges). Watch the blue text disappear as STM overflows! Type `stats` after each message to watch the dramatic overflow behavior!

### Scenario 2: LTM in Action - Knowledge Recall
**Goal**: Show LTM retrieving relevant memories based on semantic similarity

**Step-by-Step:**
```
1. User: "Tell me about honor and duty"
   ‚Üí LTM Search: Finds memories about Trial of Seven, knightly values
   ‚Üí Agent: "Aye, honor and duty are the foundation of knighthood..."
   ‚Üí Shows: Semantic retrieval of relevant memories
   ‚Üí Retrieved: "I believe a true knight must defend the helpless..."

2. User: "Who is your squire?"
   ‚Üí LTM Search: Finds memories about Egg/Prince Aegon
   ‚Üí Agent: "Ah, you speak of my squire, young Egg..."
   ‚Üí Retrieved: "I met Prince Aegon Targaryen, who I knew as Egg..."

3. User: "Tell me about a battle"
   ‚Üí LTM Search: Finds memories about Trial of Seven, Ashford
   ‚Üí Agent: Describes the Trial of Seven
   ‚Üí Retrieved: "I fought in a Trial of Seven at Ashford Meadow..."

4. User: "What happened to Prince Baelor?"
   ‚Üí LTM Search: Finds specific memory about Baelor's death
   ‚Üí Agent: "Prince Baelor Breakspear died saving my life..."
   ‚Üí Shows: Precise memory retrieval
```

**Key Observation**: Same question asked in different sessions retrieves same memories (persistence)

### Scenario 3: Hybrid Memory - STM + LTM Working Together
**Goal**: Demonstrate how both memory systems complement each other

**Step-by-Step:**
```
1. User: "Tell me about your shield"
   ‚Üí LTM: Retrieves memory about shield design
   ‚Üí Agent: "My shield bears a falling star and elm tree..."
   ‚Üí STM: Stores this exchange

2. User: "Who painted it?"
   ‚Üí STM: Knows "it" refers to shield from previous message
   ‚Üí LTM: Retrieves memory about Tanselle
   ‚Üí Agent: "Tanselle Too-Tall painted it for me at Ashford..."
   ‚Üí Shows: STM provides context, LTM provides knowledge

3. User: "Why did you defend her?"
   ‚Üí STM: Knows "her" refers to Tanselle
   ‚Üí LTM: Retrieves memory about defending Tanselle from Aerion
   ‚Üí Agent: "Prince Aerion Brightflame was cruel to her..."
   ‚Üí Shows: Perfect integration of both memory systems

4. User: "What was that prince's name again?"
   ‚Üí STM: Recalls "Prince Aerion" from recent conversation
   ‚Üí Agent: "Prince Aerion Brightflame"
   ‚Üí Shows: STM handles immediate recall
```

**Key Observation**: STM handles pronouns and references, LTM provides detailed knowledge

### Scenario 4: Memory Overflow and Persistence
**Goal**: Show STM overflow behavior and LTM persistence

**Step-by-Step:**
```
1. Start fresh session
   ‚Üí Check stats: STM: 0 messages, LTM: 20 memories

2. Have a conversation (just 4-5 exchanges needed!)
   ‚Üí Watch STM grow: 60 ‚Üí 120 ‚Üí 180 ‚Üí 220 tokens
   ‚Üí Observe: Oldest messages being removed at 200+ tokens
   ‚Üí With 200 tokens, overflow happens VERY quickly

3. Ask about something from early in conversation
   ‚Üí Agent may not remember (removed from STM after just 3-4 turns)
   ‚Üí Shows: Severe STM limitations with small buffer

4. Exit and restart the application
   ‚Üí STM: Reset to 0
   ‚Üí LTM: Still has 20 memories

5. Ask same questions about Ser Duncan's past
   ‚Üí Agent still remembers everything from LTM
   ‚Üí Shows: LTM persistence across sessions
```

**Key Observation**: With 200 tokens, you can only keep ~3-4 conversation turns in memory. This dramatically shows why LTM is critical!

### Scenario 5: Semantic Search Demonstration
**Goal**: Show how LTM finds relevant memories even with different wording

**Step-by-Step:**
```
1. User: "What's your background?"
   ‚Üí LTM finds: origin memories about Flea Bottom, Ser Arlan
   
2. User: "Where did you come from?"
   ‚Üí LTM finds: Same origin memories (semantic similarity)
   
3. User: "Tell me about your childhood"
   ‚Üí LTM finds: Same memories again
   ‚Üí Shows: Different questions, same semantic meaning

4. User: "Who taught you to fight?"
   ‚Üí LTM finds: Memories about Ser Arlan training him

5. User: "Who was your mentor?"
   ‚Üí LTM finds: Same Ser Arlan memories
   ‚Üí Shows: Semantic understanding, not keyword matching
```

**Key Observation**: LTM understands meaning, not just exact words

## Visual Memory Indicators

The web interface now includes **color-coded text** to visually distinguish between STM and LTM sources:

- **Blue text** = Information from Short-Term Memory (STM)
  - Recent conversation context
  - Immediate responses based on current dialogue
  - Disappears when STM is cleared or overflows

- **Green text** = Information from Long-Term Memory (LTM)
  - Core memories retrieved via semantic search
  - Persistent knowledge about Ser Duncan's past
  - Always available regardless of conversation length

**Legend Display:**
- A color legend appears at the top of the chat interface
- Blue box: "STM (Short-Term Memory - Recent Context)"
- Green box: "LTM (Long-Term Memory - Core Memories)"

This visual distinction makes it immediately clear to the audience which memory system is being used for each part of the response, making the demo more impactful and educational.

---

## Technical Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         User Interface (CLI/Web)        ‚îÇ
‚îÇ  - Input: User messages                 ‚îÇ
‚îÇ  - Output: Agent responses + stats      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Knight Agent                    ‚îÇ
‚îÇ  1. Receive user message                ‚îÇ
‚îÇ  2. Add to STM                          ‚îÇ
‚îÇ  3. Query LTM for relevant memories     ‚îÇ
‚îÇ  4. Build context (STM + LTM)           ‚îÇ
‚îÇ  5. Generate response                   ‚îÇ
‚îÇ  6. Add response to STM                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Memory Manager ‚îÇ  ‚îÇ  Persona Config    ‚îÇ
‚îÇ                 ‚îÇ  ‚îÇ                    ‚îÇ
‚îÇ  STM (200 tok)  ‚îÇ  ‚îÇ  - Character data  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  - 20 core mems    ‚îÇ
‚îÇ  ‚îÇ Msg 1     ‚îÇ  ‚îÇ  ‚îÇ  - Speaking style  ‚îÇ
‚îÇ  ‚îÇ Msg 2     ‚îÇ  ‚îÇ  ‚îÇ  - Backstory       ‚îÇ
‚îÇ  ‚îÇ Msg 3     ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  ‚îÇ (3-4 max) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Msg N     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ
‚îÇ  LTM (FAISS)    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Memory 1  ‚îÇ  ‚îÇ  ‚Üê Embeddings
‚îÇ  ‚îÇ Memory 2  ‚îÇ  ‚îÇ  ‚Üê Semantic search
‚îÇ  ‚îÇ ...       ‚îÇ  ‚îÇ  ‚Üê Persistent storage
‚îÇ  ‚îÇ Memory 20 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Storage Layer                   ‚îÇ
‚îÇ  - FAISS Index (Vector DB)              ‚îÇ
‚îÇ  - Metadata (Pickle files)              ‚îÇ
‚îÇ  - Sentence Transformers (Embeddings)   ‚îÇ
‚îÇ    Model: all-MiniLM-L6-v2              ‚îÇ
‚îÇ    Dimension: 384                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Memory Flow:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

User Message ‚Üí STM ‚Üí LTM Query ‚Üí Context Building ‚Üí Response
     ‚Üì           ‚Üì        ‚Üì              ‚Üì              ‚Üì
  Tokenize   Add to   Embed &      Combine STM    Generate
             buffer   Search       + LTM results   & Store
```

## Running the Demo

### CLI Version
```bash
python main.py
```
- Interactive command-line interface
- Type 'stats' to see memory statistics
- Type 'quit' to exit

### Web Version
```bash
python app.py
```
- Open browser to http://localhost:5000
- Visual interface with real-time stats
- Reset button to clear STM

## Key Talking Points

1. **Why Dual Memory?**
   - STM: Fast, maintains conversation flow
   - LTM: Persistent, semantic retrieval
   - Together: Best of both worlds

2. **Real-World Applications**
   - Customer service bots (remember customer history)
   - Personal assistants (recall user preferences)
   - Educational tutors (track learning progress)
   - Game NPCs (remember player interactions)

3. **Scalability Considerations**
   - STM: O(1) access, limited by tokens
   - LTM: O(log n) search, scales with vector DB
   - Trade-off: Speed vs. capacity

4. **Future Enhancements**
   - Memory consolidation (STM ‚Üí LTM)
   - Importance scoring
   - Memory decay/forgetting
   - Multi-modal memories (images, audio)
   - Knowledge graphs

## Q&A Preparation

**Q: Why not just use a large context window?**
A: Cost, latency, and relevance. LTM allows selective retrieval of only relevant information.

**Q: How do you decide what goes into LTM?**
A: Importance scoring based on user feedback, repetition, or explicit marking.

**Q: Can the agent forget?**
A: Yes, we can implement memory decay or explicit deletion for privacy/relevance.

**Q: How does this compare to RAG?**
A: Similar concept - LTM is essentially RAG over conversation history and persona knowledge.

## Demo Tips

### Preparation
1. Ensure `.env` file is configured (copy from `.env.example`)
2. Set `STM_MAX_TOKENS=200` for dramatic overflow demonstration
3. Have the application running before presentation
4. Open two terminals: one for app, one for monitoring
5. **Important**: With 200 tokens, overflow happens after just 3-4 exchanges - perfect for demos!

### Presentation Flow

**Phase 1: Introduction (2 minutes)**
1. Start with greeting: "Who are you?"
2. Show persona response with character details
3. Explain the Knight of Seven Kingdoms theme

**Phase 2: STM Demonstration (3 minutes)**
1. Have a natural conversation (3-4 exchanges only!)
2. Type `stats` after each message
3. Point out STM token count increasing rapidly
4. Continue until STM overflows (>200 tokens - happens fast!)
5. Show oldest messages being removed after just 3-4 turns
6. Ask about first question - agent won't remember (dramatic!)
7. Emphasize: "With only 200 tokens, we can barely keep 3-4 exchanges!"

**Phase 3: LTM Demonstration (3 minutes)**
1. Ask about character background: "Tell me about Ser Arlan"
2. Show how LTM retrieves relevant memories
3. Ask similar question differently: "Who was your mentor?"
4. Show same memories retrieved (semantic search)
5. Type `stats` to show LTM count (25 memories)

**Phase 4: Hybrid Memory (3 minutes)**
1. Ask: "Tell me about your shield"
2. Follow up: "Who painted it?" (pronoun from STM)
3. Follow up: "Why did you defend her?" (context from STM + LTM)
4. Show how both systems work together

**Phase 5: Persistence (2 minutes)**
1. Exit the application
2. Restart it
3. Show STM reset to 0
4. Ask same questions - LTM still remembers
5. Demonstrate persistence across sessions

### Key Points to Emphasize

**STM Benefits:**
- ‚úÖ Fast access (O(1))
- ‚úÖ Maintains conversation flow
- ‚úÖ Handles pronouns and references
- ‚ùå Very limited capacity (200 tokens = ~3-4 exchanges)
- ‚ùå Loses old context quickly

**LTM Benefits:**
- ‚úÖ Unlimited capacity (scales with disk)
- ‚úÖ Semantic search (understands meaning)
- ‚úÖ Persistent across sessions
- ‚úÖ Organized by categories
- ‚ùå Slower than STM (embedding + search)

**Hybrid Approach:**
- ‚úÖ Best of both worlds
- ‚úÖ STM for immediate context
- ‚úÖ LTM for knowledge retrieval
- ‚úÖ Realistic conversation experience

### Common Questions & Answers

**Q: Why only 200 tokens for STM?**
A: This is intentionally small for demonstration purposes! It makes overflow behavior very visible - you can see memory management in action after just 3-4 exchanges. In production, you'd use 2000-4000 tokens depending on use case. The small size dramatically shows why LTM is essential.

**Q: How many conversation turns fit in 200 tokens?**
A: Approximately 3-4 exchanges (user message + agent response). Each exchange averages 50-70 tokens, so 200 tokens fills up very quickly!

**Q: How does semantic search work?**
A: User query ‚Üí Embedding (384-dim vector) ‚Üí Cosine similarity with stored embeddings ‚Üí Top-K results

**Q: What if STM and LTM conflict?**
A: STM takes precedence for immediate context. LTM provides background knowledge.

**Q: Can you add new memories to LTM?**
A: Yes! Important conversations can be promoted from STM to LTM with importance scoring.

**Q: How much does this cost?**
A: Embeddings are local (free). Only cost is storage (minimal) and compute (negligible for small scale).

### Troubleshooting

**Issue**: Agent responses are generic
- **Fix**: Check that LTM loaded properly (should have 20 memories)
- **Command**: Type `stats` to verify

**Issue**: STM not overflowing
- **Fix**: With 200 tokens, this shouldn't happen! Just 3-4 exchanges should trigger overflow
- **Check**: Verify STM_MAX_TOKENS=200 in .env file

**Issue**: Same memories retrieved every time
- **Fix**: This is correct! Shows consistency and reliability

### Advanced Demonstrations

**Memory Categories:**
```bash
# Show different memory types
"Tell me about your origin"     ‚Üí origin memories
"What defines you?"             ‚Üí defining_moment memories  
"Who do you know?"              ‚Üí relationships memories
"What do you believe?"          ‚Üí values memories
```

**Token Counting:**
```bash
# Show token efficiency
Short message: "Hi" ‚Üí ~5 tokens
Medium message: "Tell me about honor" ‚Üí ~20 tokens
Long response: Full paragraph ‚Üí ~100-150 tokens
```

**Semantic Similarity:**
```bash
# Same meaning, different words
"Who taught you?" 
"Who was your teacher?"
"Who trained you?"
‚Üí All retrieve Ser Arlan memories
```

## Contact & Resources

- GitHub: [Your Repo URL]
- Demo: ACD Ahmedabad 2026
- Tech Stack: Python, ChromaDB, Flask, Sentence Transformers
